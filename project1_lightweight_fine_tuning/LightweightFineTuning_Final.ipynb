{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: LoRa - low-rank adaption, LoRa adds low-rank matrics to the model's layers, reducing the number of parameters that need to be updated\n",
    "* Model: gpt-2, A transformer-based model, it's relatively lighweight compared to other large models for sequence classification\n",
    "* Evaluation approach: Hugging Face Trainer API with Compute Metrics (acuracy-based) to assess fine-tuning performance\n",
    "* Fine-tuning dataset: IMDB dataset for sentiment classification, a popular dataset with movie reviews labeled as positive or negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6029cf",
   "metadata": {},
   "source": [
    "<a name='hugging-face-peft-library'>Hugging Face PEFT Library</a>:\n",
    "\n",
    "<ul>\n",
    "  <li><a href=\"https://huggingface.co/docs/peft/index\" style=\"text-decoration:none\">What is PEFT?</a>: PEFT is a method for fine-tuning large language models with a focus on efficiency. Traditional fine-tuning requires modifying and storing all the model parameters, which can be extremely resource-intensive, especially with very large models. PEFT aims to reduce the amount of memory and computation required by fine-tuning only a small subset of parameters, instead of the entire model.</li>\n",
    "  <li><a href=\"https://huggingface.co/docs/peft/main/en/conceptual_guides/lora\" style=\"text-decoration:none\">Using LoRA as a PEFT Technique</a>:LoRA is a specific PEFT technique that uses low-rank matrix adaptation to achieve efficient fine-tuning without altering the core model parameters. </li>\n",
    "   <li><a href=\"https://huggingface.co/docs/bitsandbytes/main/en/index\" style=\"text-decoration:none\">What is QLoRa?</a>:QLoRA (Quantized LoRA) combines quantization with LoRA to further reduce the memory footprint and computation costs. Quantization is a process where the model's weights are converted from higher-precision floating-point numbers (like FP16 or FP32) to lower-precision numbers (like 4-bit or 8-bit integers). This reduces the model's size and speeds up computations, while LoRA provides a way to perform fine-tuning on a small subset of parameters (low-rank matrices).</li>\n",
    "    <li><a style=\"text-decoration:none\">Why use QLoRa with bitsandbytes?</a>:Using QLoRA with bitsandbytes for inference instead of LoRA allows you to leverage both quantization and low-rank adaptation. This combination offers significant memory and speed advantages, enabling large language models to be efficiently deployed on limited hardware while maintaining high accuracy and performance.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f551c63a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.36.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.5.0)\n",
      "Requirement already satisfied: evaluate in /home/student/.local/lib/python3.10/site-packages (0.4.3)\n",
      "Requirement already satisfied: scikit-learn in /home/student/.local/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.41.1)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.25.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/student/.local/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/student/.local/lib/python3.10/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/student/.local/lib/python3.10/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/student/.local/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/student/.local/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: requests in /home/student/.local/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: filelock in /home/student/.local/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /home/student/.local/lib/python3.10/site-packages (from datasets) (2024.2.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: pandas in /home/student/.local/lib/python3.10/site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/student/.local/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/student/.local/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/student/.local/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/student/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/student/.local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/student/.local/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/student/.local/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/student/.local/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/student/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/student/.local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/student/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/student/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/student/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/student/.local/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets peft evaluate scikit-learn bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4935cb4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "model_name='gpt2'\n",
    "dataset=load_dataset('imdb')\n",
    "#load a smaller portion of the dataset by using the shuffle() and select() methods to take a random sample\n",
    "sample_size=5000\n",
    "train_dataset = dataset['train'].shuffle(seed=12).select(range(sample_size))\n",
    "test_dataset = dataset['test'].shuffle(seed=12).select(range(sample_size))\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name,num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ea17f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "809b6926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 5000\n",
      "Test dataset size: 5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': ['Buffs of the adult western that flourished in the 1950s try and trace its origins to the film that kicked off the syndrome. Of course, we can go back to Howard Hawks\\'s Red River (1948) or further still to John Ford\\'s My Darling Clementine (1946), but if we want to stick with this single decade, then it has to be one of a couple of films made in that era\\'s initial year. One is \"The Gunfighter,\" an exquisitely grim tale of a famed gunslinger (Ringo) facing his last shootout. Another from that same year is \"Winchester \\'73,\" and it\\'s worth noting that Millard Mitchell appears in both as grim, mustached, highly realistic range riders. In The Gunfighter, he\\'s the town marshal expected to arrest Ringo but once rode with him in an outlaw gang. In Winchester, he\\'s the sidekick to Jimmy Stewart, a kind of Horatio to Stewart\\'s Hamlet in this epic/tragic tale. The plot is simple enough: Stewart\\'s lonesome cowpoke wins a remarkable Winchester in a shooting match, beating the meanest man in the west (Stephen McNally), who is actually his own brother and caused the death of their father. When the brother steals the gun, Stewart and Mitchell go after him in a cowboy odyssey that takes them all across the frontier, meeting up with both outlaws and Indians. (In one wonderful bit, two future stars - Rock Hudson and Tony Curtis - play an Indian chief and a U.S. cavalry soldier - during a well staged pitched-battle. Dan Duryea steals the whole show as a giggling outlaw leader, while Shelly Winters, just before she began to gain weight, is fine as the shady lady who ties all the plots together. Today, filmmakers would go on for about four hours to bring such an ambitious idea to the screen, but Anthony Mann does so in an extremely economical amount of time, with not a minute wasted. Such western legends as Bat Masterson and Wyatt Earp (terrifically played by Will Geer) make brief appearances, adding to the historicity as well as the epic nature. The final battle between good and bad brothers, high atop a series of jutting rock canyons, is now legendary among western buffs. It\\'s also worth noting that Stewart, however much associated he became with western films, does what is actually his first western leading man role here - yes, he was in Destry Rides Again eleven years earlier, but was cast in that comedy spoof because he seemed so WRONG for westerns!'],\n",
       " 'label': [1]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Training dataset size: {len(train_dataset)}')\n",
    "print(f'Test dataset size: {len(test_dataset)}')\n",
    "train_dataset[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cef86439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({1: 2467, 0: 2533}), Counter({1: 2467, 0: 2533}))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for data imbalance\n",
    "from collections import Counter\n",
    "Counter(train_dataset['label']), Counter(test_dataset['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df23213",
   "metadata": {},
   "source": [
    "<a>data is balanced.</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68bf8b3f9534539aaf076264d5a50e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709a57fe8c344371a4569130d1df5515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_data(sample):\n",
    "    return tokenizer(sample['text'],truncation=True, padding='max_length', max_length=128)\n",
    "tokenizer.pad_token=tokenizer.eos_token  #end-of-sequence token\n",
    "# tokenizer.add_special_tokens({'pad_token':'[PAD]'})\n",
    "#ensure the model configuration recognizes the padding token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "train_dataset = train_dataset.map(tokenize_data, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_data, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming the column from label to labels is necessary to ensure compatibility with the HF Trainer API>=\n",
    "train_dataset = train_dataset.rename_column('label','labels')\n",
    "test_dataset = test_dataset.rename_column('label','labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d029a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "accuracy_metric= load('accuracy')\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    #pred is generated by the Trainer during evaluation.  \n",
    "    #pred.label_ids: the true labels of the dataset.\n",
    "    # pred.predictions: the model's raw output predictions for each sample in the dataset.\n",
    "    labels = pred.label_ids  #extract labels\n",
    "    #converts the raw model predictions into actual class predictions by taking the index with the highest probability\n",
    "    preds = pred.predictions.argmax(-1) \n",
    "    #return a dictionary with the accuracy score\n",
    "    return accuracy_metric.compute(predictions=preds, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02b982ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 01:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial evaluation results:  {'eval_loss': 0.8709287047386169, 'eval_accuracy': 0.4792, 'eval_runtime': 76.8252, 'eval_samples_per_second': 65.083, 'eval_steps_per_second': 65.083}\n"
     ]
    }
   ],
   "source": [
    "training_args=TrainingArguments(output_dir='outputs',evaluation_strategy='epoch',\n",
    "                                logging_steps=10,report_to='all', per_device_eval_batch_size=1)\n",
    "#creating the trainer\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset, #generating pred: preditions and label_ids\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "#initial evaluation\n",
    "initial_results = trainer.evaluate()  #calls the model on eval_dataset\n",
    "print('Initial evaluation results: ', initial_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/tuners/lora.py:475: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha = 20,  #increaing alpha boosts the effect of the new parameters, while lowering it reduces their impact.\n",
    "    lora_dropout=0.1,  #preventing overfitting by randomly dropping some connections during trainings\n",
    "    r = 4,  #lower ranks reduce the number of new parameters added to the model, it can speed up training\n",
    "    task_type='SEQ_CLS', # ensure the fine-tuning aligns with sequence classification needs\n",
    "    bias='lora_only' #only bias associated with LoRa layers are trainable\n",
    ")\n",
    "#set up the BitsAndBytesConfig for 4-bit quantization\n",
    "# quantization_config = BitsAndBytesConfig(load_in_8bit=True,bnb_8bit_compute_dtype=torch.float16)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name,num_labels=2, \n",
    "#                                                           device_map='auto',\n",
    "#                                                           quantization_config=quantization_config)\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id\n",
    "peft_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "894046c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1875/1875 09:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.595400</td>\n",
       "      <td>0.641472</td>\n",
       "      <td>0.634600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.293300</td>\n",
       "      <td>0.388185</td>\n",
       "      <td>0.836200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.443700</td>\n",
       "      <td>0.380486</td>\n",
       "      <td>0.838000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory outputs/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory outputs/checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory outputs/checkpoint-1500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    }
   ],
   "source": [
    "#train the LoRa model\n",
    "trainer = Trainer(\n",
    "    model = peft_model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "#starts the fine-tuning process, updating only the parameters introduced by LoRa \n",
    "#while keeping the core pre-trained model mostly unchanged\n",
    "# duing training, the model learns the task-specific patterns in the training data \n",
    "trainer.train()\n",
    "#save the fine-tuned model\n",
    "peft_model.save_pretrained('fine_tuned_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26005ec",
   "metadata": {},
   "source": [
    "<a name='notes-about-peft-model'> Why need to use PEFT-specific model class? </a>\n",
    "* The AutoPeftModelForSequenceClassification if specifically designed to load models fine-tuned with PEFT, like LoRa,QLoRa. This is necessary because PEFT methods like LoRA only save and load adapter weights, not the full model weights.\n",
    "* Using this PEFT-specific class ensures the model knows to use the adapter weights (rather than expecting full model weights) and lets you perform efficient inference with your lightweight, fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 01:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model evaluation results:  {'eval_loss': 0.4047755002975464, 'eval_accuracy': 0.8348, 'eval_runtime': 88.5558, 'eval_samples_per_second': 56.462, 'eval_steps_per_second': 56.462}\n"
     ]
    }
   ],
   "source": [
    "#load fine-tuned model\n",
    "from peft import AutoPeftModelForSequenceClassification\n",
    "#loading a saved PEFT model\n",
    "peft_model = AutoPeftModelForSequenceClassification.from_pretrained('fine_tuned_model')\n",
    "#update the trainer with the fine-tuned model\n",
    "trainer = Trainer(\n",
    "    model = peft_model,\n",
    "    args = training_args,\n",
    "    eval_dataset = test_dataset,\n",
    "    compute_metrics = compute_metrics,\n",
    ")\n",
    "\n",
    "#final evaluation\n",
    "tuned_results = trainer.evaluate()\n",
    "print('Fine-tuned model evaluation results: ', tuned_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initail model accuracy:  0.4792\n",
      "Fine-tuned model accuracy:  0.8348\n"
     ]
    }
   ],
   "source": [
    "print('Initail model accuracy: ', initial_results['eval_accuracy'])\n",
    "print('Fine-tuned model accuracy: ', tuned_results['eval_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
