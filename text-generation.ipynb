{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from helper import (\n",
    "    start_time,\n",
    "    time_since,\n",
    "    ShakespeareDataset,\n",
    "    TokenMapping,\n",
    "    build_model,\n",
    "    next_token,\n",
    "    # Character-based helpers\n",
    "    encode_text,\n",
    "    # Subword-based helpers\n",
    "    encode_text_from_tokenizer,\n",
    "    tokenize_text_from_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Deterministic training\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Attempt GPU; if not, stay on CPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in text file: 50,085\n"
     ]
    }
   ],
   "source": [
    "# Reduced data to make it manageable for smaller systems\n",
    "DATA_FILE: str = '../data/shakespeare_small.txt'\n",
    "\n",
    "with open(DATA_FILE, 'r') as data_file:\n",
    "    raw_text = data_file.read()\n",
    "\n",
    "print(f'Number of characters in text file: {len(raw_text):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-Based Text Generation\n",
    "\n",
    "The first model you'll build for text generation will use character-based\n",
    "tokens.\n",
    "\n",
    "Each token will be a single character from the text and the model will learn\n",
    "to predict the next character (a token).\n",
    "\n",
    "To generate text, the model will take in a new string,\n",
    "character-by-character, and then generate a new likely character based on the\n",
    "past input. Then the model will take into account that new character and\n",
    "generate the following character and so on and so on until the model has\n",
    "produced a set number of characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Text into Integer Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefor'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def normalize_text(text: str) -> str:\n",
    "    # TODO: Normalize incoming text; can be multiple actions\n",
    "#     s1 = text.lower()\n",
    "#     s2 = re.sub(r'[^a-z0-9]',' ', s1) #remove special charaters, punctuation\n",
    "#     s3 = re.sub(r'\\s+',' ', s2)  #turn multiple spaces into a single space\n",
    "#     normalized_text = s3.strip() #remove whitespaces\n",
    "    \n",
    "    normalized_text =  text.lower()\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first citizen:\n",
      "before we proceed any further, hear me speak.\n",
      "\n",
      "all:\n",
      "speak, speak.\n",
      "\n",
      "first citizen:\n",
      "you are all resolved rather to die than to famish?\n",
      "\n",
      "all:\n",
      "resolved. resolved.\n",
      "\n",
      "first citizen:\n",
      "first, you know caius marcius is chief enemy to the people.\n",
      "\n",
      "all:\n",
      "we know't, we know't.\n",
      "\n",
      "first citizen:\n",
      "let us kill him, and we'll have corn at our own price.\n",
      "is't a verdict?\n",
      "\n",
      "all:\n",
      "no more talking on't; let it be done: away, away!\n",
      "\n",
      "second citizen:\n",
      "one word, good citizens.\n",
      "\n",
      "first citizen:\n",
      "we are accounted poor\n"
     ]
    }
   ],
   "source": [
    "# TEST: Is your text normalized the way you expected?\n",
    "# Only the first 500 characters of the original text\n",
    "normalized_text = normalize_text(raw_text[:500])\n",
    "print(normalized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretokenize_text(text: str) -> str | list[str]:\n",
    "    # TODO: Pretokenize normalized text into character strings\n",
    "    smaller_pieces = [char for char in text]\n",
    "    return smaller_pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'b', 'e', 'f', 'o', 'r', 'e', ' ', 'w', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 'e', 'd', ' ', 'a', 'n', 'y', ' ', 'f', 'u', 'r', 't', 'h', 'e', 'r', ',', ' ', 'h', 'e', 'a', 'r', ' ', 'm', 'e', ' ', 's', 'p', 'e', 'a', 'k', '.', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 's', 'p', 'e', 'a', 'k', ',', ' ', 's', 'p', 'e', 'a', 'k', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'y', 'o', 'u', ' ', 'a', 'r', 'e', ' ', 'a', 'l', 'l', ' ', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', ' ', 'r', 'a', 't', 'h', 'e', 'r', ' ', 't', 'o', ' ', 'd', 'i', 'e', ' ', 't', 'h', 'a', 'n', ' ', 't', 'o', ' ', 'f', 'a', 'm', 'i', 's', 'h', '?', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', '.', ' ', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'f', 'i', 'r', 's', 't', ',', ' ', 'y', 'o', 'u', ' ', 'k', 'n', 'o', 'w', ' ', 'c', 'a', 'i', 'u', 's', ' ', 'm', 'a', 'r', 'c', 'i', 'u', 's', ' ', 'i', 's', ' ', 'c', 'h', 'i', 'e', 'f', ' ', 'e', 'n', 'e', 'm', 'y', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 'p', 'e', 'o', 'p', 'l', 'e', '.', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 'w', 'e', ' ', 'k', 'n', 'o', 'w', \"'\", 't', ',', ' ', 'w', 'e', ' ', 'k', 'n', 'o', 'w', \"'\", 't', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'l', 'e', 't', ' ', 'u', 's', ' ', 'k', 'i', 'l', 'l', ' ', 'h', 'i', 'm', ',', ' ', 'a', 'n', 'd', ' ', 'w', 'e', \"'\", 'l', 'l', ' ', 'h', 'a', 'v', 'e', ' ', 'c', 'o', 'r', 'n', ' ', 'a', 't', ' ', 'o', 'u', 'r', ' ', 'o', 'w', 'n', ' ', 'p', 'r', 'i', 'c', 'e', '.', '\\n', 'i', 's', \"'\", 't', ' ', 'a', ' ', 'v', 'e', 'r', 'd', 'i', 'c', 't', '?', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 'n', 'o', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'a', 'l', 'k', 'i', 'n', 'g', ' ', 'o', 'n', \"'\", 't', ';', ' ', 'l', 'e', 't', ' ', 'i', 't', ' ', 'b', 'e', ' ', 'd', 'o', 'n', 'e', ':', ' ', 'a', 'w', 'a', 'y', ',', ' ', 'a', 'w', 'a', 'y', '!', '\\n', '\\n', 's', 'e', 'c', 'o', 'n', 'd', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'o', 'n', 'e', ' ', 'w', 'o', 'r', 'd', ',', ' ', 'g', 'o', 'o', 'd', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', 's', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'w', 'e', ' ', 'a', 'r', 'e', ' ', 'a', 'c', 'c', 'o', 'u', 'n', 't', 'e', 'd', ' ', 'p', 'o', 'o', 'r']\n"
     ]
    }
   ],
   "source": [
    "# TEST: Is your (normalized) text pretokenized the way you expected?\n",
    "# Only the first 500 characters of the original text\n",
    "pretokenized_text = pretokenize_text(normalized_text)\n",
    "print(pretokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine normalization and pretokenization steps\n",
    "def tokenize_text(text: str) -> str | list[str]:\n",
    "    normalized_text: str = normalize_text(text)\n",
    "    pretokenized_text: str | list[str] = pretokenize_text(normalized_text)\n",
    "    # Characters are already tokens so pretokenized text is already tokenized\n",
    "    tokenized_text = pretokenized_text\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'b', 'e', 'f', 'o', 'r', 'e', ' ', 'w', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 'e', 'd', ' ', 'a', 'n', 'y', ' ', 'f', 'u', 'r', 't', 'h', 'e', 'r', ',', ' ', 'h', 'e', 'a', 'r', ' ', 'm', 'e', ' ', 's', 'p', 'e', 'a', 'k', '.', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 's', 'p', 'e', 'a', 'k', ',', ' ', 's', 'p', 'e', 'a', 'k', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'y', 'o', 'u', ' ', 'a', 'r', 'e', ' ', 'a', 'l', 'l', ' ', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', ' ', 'r', 'a', 't', 'h', 'e', 'r', ' ', 't', 'o', ' ', 'd', 'i', 'e', ' ', 't', 'h', 'a', 'n', ' ', 't', 'o', ' ', 'f', 'a', 'm', 'i', 's', 'h', '?', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', '.', ' ', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'f', 'i', 'r', 's', 't', ',', ' ', 'y', 'o', 'u', ' ', 'k', 'n', 'o', 'w', ' ', 'c', 'a', 'i', 'u', 's', ' ', 'm', 'a', 'r', 'c', 'i', 'u', 's', ' ', 'i', 's', ' ', 'c', 'h', 'i', 'e', 'f', ' ', 'e', 'n', 'e', 'm', 'y', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 'p', 'e', 'o', 'p', 'l', 'e', '.', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 'w', 'e', ' ', 'k', 'n', 'o', 'w', \"'\", 't', ',', ' ', 'w', 'e', ' ', 'k', 'n', 'o', 'w', \"'\", 't', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'l', 'e', 't', ' ', 'u', 's', ' ', 'k', 'i', 'l', 'l', ' ', 'h', 'i', 'm', ',', ' ', 'a', 'n', 'd', ' ', 'w', 'e', \"'\", 'l', 'l', ' ', 'h', 'a', 'v', 'e', ' ', 'c', 'o', 'r', 'n', ' ', 'a', 't', ' ', 'o', 'u', 'r', ' ', 'o', 'w', 'n', ' ', 'p', 'r', 'i', 'c', 'e', '.', '\\n', 'i', 's', \"'\", 't', ' ', 'a', ' ', 'v', 'e', 'r', 'd', 'i', 'c', 't', '?', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 'n', 'o', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'a', 'l', 'k', 'i', 'n', 'g', ' ', 'o', 'n', \"'\", 't', ';', ' ', 'l', 'e', 't', ' ', 'i', 't', ' ', 'b', 'e', ' ', 'd', 'o', 'n', 'e', ':', ' ', 'a', 'w', 'a', 'y', ',', ' ', 'a', 'w', 'a', 'y', '!', '\\n', '\\n', 's', 'e', 'c', 'o', 'n', 'd', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'o', 'n', 'e', ' ', 'w', 'o', 'r', 'd', ',', ' ', 'g', 'o', 'o', 'd', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', 's', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'w', 'e', ' ', 'a', 'r', 'e', ' ', 'a', 'c', 'c', 'o', 'u', 'n', 't', 'e', 'd', ' ', 'p', 'o', 'o', 'r']\n"
     ]
    }
   ],
   "source": [
    "# TEST: Is your tokenized text the way you expected?\n",
    "tokenized_text = tokenize_text(raw_text[:500])\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocessing\n",
    "\n",
    "We'll skip postprocessing since we don't have any special tokens we want to\n",
    "consider for our task here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode (Tokens → Integer IDs)\n",
    "\n",
    "We have `encode_text()` from our helper module that can encode our text based on\n",
    "our tokenization process from our created `tokenize_text()` function.\n",
    "\n",
    "This will also provide us with `character_mapping`, an object that we can use to\n",
    "map our tokens back and forth from strings to integer IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text, character_mapping = encode_text(raw_text, tokenize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 50,086 characters\n"
     ]
    }
   ],
   "source": [
    "n_tokens = character_mapping.n_tokens\n",
    "dataset_size = len(encoded_text)\n",
    "print(f'Size of dataset: {dataset_size:,} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining sequence length that will be taken in at a time by our model\n",
    "sequence_length = 32 # Number of characters\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = ShakespeareDataset(encoded_text, sequence_length)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=False, # Ensure deterministic training\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "\n",
    "We'll provide a defined model today, but this could be a step that you would\n",
    "modify and experiment in other NLP projects you'll do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model to be trained and generate text with\n",
    "model = build_model(n_tokens)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Text Generation\n",
    "\n",
    "The `generate_text_by_char()` function will use your tokenizer and NLP model to\n",
    "generate new text token-by-token (character-by-character in this case) by taking\n",
    "in the input text and token sampling parameters.\n",
    "\n",
    "We can use temperature and top-k sampling to adjust the \"creativeness\" of the\n",
    "generated text.\n",
    "\n",
    "We also pass in the `num_chars` parameter to tell the function how many tokens\n",
    "(characters in this case) to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_by_char(\n",
    "    input_str: str,\n",
    "    model,\n",
    "    token_mapping: TokenMapping = character_mapping,\n",
    "    num_chars: int = 100,\n",
    "    temperature: float = 1.0,\n",
    "    topk: int | None = None,\n",
    ") -> str:\n",
    "    # Uses your character-based tokenizer\n",
    "    tokenized_text: list[str] = tokenize_text(input_str)\n",
    "    # Generates token-by-token and creates a list of those tokens\n",
    "    generated_tokens = []\n",
    "    for _ in range(num_chars):\n",
    "        # Uses the input text and generated text (so far) to get next token\n",
    "        new_char = next_token(\n",
    "            tokenized_text=(tokenized_text + generated_tokens),\n",
    "            model=model,\n",
    "            token_mapping=token_mapping,\n",
    "            # Temperature & top-k sampling used in determining the next token\n",
    "            temperature=temperature,\n",
    "            topk=topk,\n",
    "            device=device,\n",
    "        )\n",
    "        generated_tokens.append(new_char)\n",
    "    # Returns input string plus the full generated string (of generated tokens)\n",
    "    full_text = ''.join(tokenized_text + generated_tokens)\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "At this point, the model has not been trained so the code below will train the\n",
    "NLP model that will be used to generate new text.\n",
    "\n",
    "The model will take in the text data (broken by tokens by our character-based\n",
    "tokenizer) and attempt to predict the next token. Over time, the model should\n",
    "hopefully get better in predicting the next token (given the previous text).\n",
    "\n",
    "To help us visualize how the model is training, at the end of every epoch, we\n",
    "generate text using the `TEST_PHRASE` with the improving model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 2.530167247464482\n",
      "[00m 4.7s (0 0.0) 2.1864]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bee and vevenptury havir\n",
      "if:\n",
      "comums mey:\n",
      "oucidy\n",
      " irastr mavif,\n",
      "met to gow ow, it!citisere-TOKEN_NOT_FOUNDctim thongi\n",
      "Epoch 2/25, Loss: 2.181258435523548\n",
      "[00m 9.4s (1 4.0) 1.9870]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bem fiese have an, ato shind thai daraster harfe uss peiind aftorfer;\n",
      "hins, inius:\n",
      "ine deart haet of o\n",
      "Epoch 3/25, Loss: 2.0785378108771084\n",
      "[00m 14.0s (2 8.0) 1.8850]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bevis sinsnores\n",
      "in coak foar.\n",
      "there cont rye sel cot cim andp, gry llott hal and aliendry no; pirse mo\n",
      "Epoch 4/25, Loss: 2.019213537819469\n",
      "[00m 18.6s (3 12.0) 1.8197]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be thies and the bericenyly he 'ut nopher iaret; do ltim uncius:iatrris sotry cofa'linius:\n",
      "olonger pat\n",
      "Epoch 5/25, Loss: 1.9766919984604223\n",
      "[00m 23.2s (4 16.0) 1.7796]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to beint heserusss art\n",
      "the, hond ashing\n",
      "prechak covtod. foud.\n",
      "\n",
      "ises\n",
      "the pratity, aloves,\n",
      "the mo:\n",
      "such wor\n",
      "Epoch 6/25, Loss: 1.9433630841989487\n",
      "[00m 27.8s (5 20.0) 1.7546]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bet turem he dow, will wrupuds:\n",
      "susther ofece you no afder o'd-ldreg tro gatendy!\n",
      "\n",
      "mart dailice? 'tour\n",
      "Epoch 7/25, Loss: 1.9164596149334892\n",
      "[00m 32.5s (6 24.0) 1.7339]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to beir the suces comes our fice\n",
      "lese he bald withis the grution the depons atpliantry vrat, your for ed \n",
      "Epoch 8/25, Loss: 1.8943748663027828\n",
      "[00m 37.2s (7 28.000000000000004) 1.7176]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bet horeto, wond hond he,\n",
      "plard you parscore bean:\n",
      "yet him no love poven here shownow?\n",
      "hact troth be s\n",
      "Epoch 9/25, Loss: 1.8757074330942318\n",
      "[00m 41.9s (8 32.0) 1.7068]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be t.\n",
      "\n",
      "first ito wheleqee\n",
      "you a\n",
      "knor hubleoger therie for, their huth im,\n",
      "apcond deest nor eas.\n",
      "\n",
      "menen\n",
      "Epoch 10/25, Loss: 1.859731473937964\n",
      "[00m 46.6s (9 36.0) 1.6993]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be\n",
      "fort sheren otty dreed betor exves ditizens\n",
      "to jindes, grtith on to see,\n",
      "forth temegs semarg\n",
      "and hi\n",
      "Epoch 11/25, Loss: 1.8458594787615938\n",
      "[00m 51.1s (10 40.0) 1.6941]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to beirs, anded be gon bus unders pites, lovore and titcusn gook suctighon. hin\n",
      "at me of my esterang and \n",
      "Epoch 12/25, Loss: 1.8337553211675284\n",
      "[00m 55.7s (11 44.0) 1.6896]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to beserot, terul this knower for so the, of wratel.\n",
      "\n",
      "bul worf know at again broughll fith\n",
      "thopvatu, the \n",
      "Epoch 13/25, Loss: 1.8230699543754894\n",
      "[01m 0.4s (12 48.0) 1.6847]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to belkgol!\n",
      "\n",
      "distrects plakat thiceady gright and pet and his and their prerbe cas mite qepore ofry, pret\n",
      "Epoch 14/25, Loss: 1.8135556534075508\n",
      "[01m 5.0s (13 52.0) 1.6784]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to betortus, 'thich its foel hent to s'le\n",
      "it could\n",
      "cootle\n",
      "you suceng.\n",
      "\n",
      "maved the masing in: iwtl. powf th\n",
      "Epoch 15/25, Loss: 1.805094664013043\n",
      "[01m 9.6s (14 56.00000000000001) 1.6712]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be coumby\n",
      "thear blusted.\n",
      "\n",
      "merpromus: whong.\n",
      "\n",
      "coriolia:\n",
      "i tike thetcen a cam, to with ofrest speeced yo\n",
      "Epoch 16/25, Loss: 1.7974912732553938\n",
      "[01m 14.2s (15 60.0) 1.6645]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to becenue, bumswell fraderssy too ssim'sy, this all:\n",
      "\n",
      "flitare yet peath\n",
      "ind poor dof too secrone the wig\n",
      "Epoch 17/25, Loss: 1.7906103220991434\n",
      "[01m 18.8s (16 64.0) 1.6590]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be no' ear the apy the vourse\n",
      "of and and flay olal this therhe praven it this heporter;, is by trummar\n",
      "Epoch 18/25, Loss: 1.7843518001964678\n",
      "[01m 23.4s (17 68.0) 1.6552]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be the ladk so with, come;\n",
      "ficeind tipopticificore to oarticia\n",
      "ricinily of, when an\n",
      "and the mus cordit\n",
      "Epoch 19/25, Loss: 1.7786252130715612\n",
      "[01m 28.0s (18 72.0) 1.6521]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to besteed their solia:\n",
      "the and,\n",
      "in withes seleld that his at hond inged prithink\n",
      "for at hargutius home\n",
      "d\n",
      "Epoch 20/25, Loss: 1.773372413556035\n",
      "[01m 32.6s (19 76.0) 1.6488]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bent thiunst could he its, a knone,\n",
      "him so foar knot prodle.\n",
      "\n",
      "knot; thund eveeds letientitiuld a mome \n",
      "Epoch 21/25, Loss: 1.7685355813358539\n",
      "[01m 37.2s (20 80.0) 1.6451]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to besest word\n",
      "to entwetb, of his.\n",
      "\n",
      "marcius, no sird: to spetter time 'twight velrugition. for, no woulf \n",
      "Epoch 22/25, Loss: 1.7640523426068095\n",
      "[01m 41.8s (21 84.0) 1.6418]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be\n",
      "and rome?\n",
      "\n",
      "nand shoutty'd of beir the coriinius:\n",
      "hate have sever staths to the to tes. one thoid\n",
      "sh\n",
      "Epoch 23/25, Loss: 1.7598620898426531\n",
      "[01m 46.3s (22 88.0) 1.6392]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be he husool.\n",
      "\n",
      "laty\n",
      "hove\n",
      "a to kfigh, by see poif trop ono mad teint to preve merefold\n",
      "to make plater e\n",
      "Epoch 24/25, Loss: 1.7559759692262156\n",
      "[01m 50.9s (23 92.0) 1.6372]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bed juted,\n",
      "ifs to stimns\n",
      "at har.\n",
      "\n",
      "mareslen we a praver, furfum,\n",
      "i\n",
      "for robmy. the be o' the thees diemy\n",
      "Epoch 25/25, Loss: 1.7523777404913126\n",
      "[01m 55.5s (24 96.0) 1.6360]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bet they sconblew here of and fare, curpen the margith,\n",
      "i be heebe\n",
      "and as:\n",
      "theref not fon, wolsul of h\n"
     ]
    }
   ],
   "source": [
    "TEST_PHRASE = 'To be or not to be'\n",
    "# Use more epochs if not CPU device\n",
    "epochs = 5 if device == 'cpu' else 25\n",
    "\n",
    "start = start_time()\n",
    "for epoch in range(epochs):\n",
    "    # Set model into \"training mode\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch.to(device))\n",
    "        loss = criterion(output.transpose(1, 2), y_batch.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}')\n",
    "    print(f'[{time_since(start)} ({epoch} {epoch / epochs * 100}) {loss:.4f}]')\n",
    "    print('-'*72)\n",
    "    gen_output = generate_text_by_char(\n",
    "        input_str=TEST_PHRASE,\n",
    "        model=model,\n",
    "        num_chars=100,\n",
    "    )\n",
    "    print(gen_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text\n",
    "\n",
    "Now that the model has been trained, go ahead and observe how it performs!\n",
    "\n",
    "Try adjusting the different sampling methods using the `temperature` and `topk`\n",
    "parameters on the same input string to see the differences.\n",
    "\n",
    "You might also try different phrases as well as how many tokens (`num_chars`) to\n",
    "generate and observe how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be or not to being to the go put all scosius!\n",
      "it you will end bery are here of their as your canstegred\n",
      "for huld ba\n"
     ]
    }
   ],
   "source": [
    "output = generate_text_by_char(\n",
    "    input_str='To be or not to be',\n",
    "    model=model,\n",
    "    num_chars=100,\n",
    "    temperature=1.0,\n",
    "    topk=None,\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be or not to be sus mlo spagars. too spenger.' husgorys:-walicu-y-'ffercicesiso,!ns',; wwubradinhug,nc:nia wie's se\n"
     ]
    }
   ],
   "source": [
    "output = generate_text_by_char(\n",
    "    input_str='To be or not to be',\n",
    "    model=model,\n",
    "    num_chars=100,\n",
    "    temperature=15,\n",
    "    topk=10,\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be or not to be the prould the prould the prould the prould the prould the prould the prould the prould the prould the prould the prould the prould the prould the prould the prould the prould the prould the prould t\n"
     ]
    }
   ],
   "source": [
    "output = generate_text_by_char(\n",
    "    input_str='To be or not to be',\n",
    "    model=model,\n",
    "    num_chars=200,\n",
    "    temperature=0.00005,\n",
    "    topk=1,\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subword Text Generation\n",
    "\n",
    "The next model you'll build will use subword-tokenization instead of \n",
    "characters-based token to train a model and ultimately generate new text\n",
    "token-by-token.\n",
    "\n",
    "Although this could be done by creating your own tokenizer, you'll use\n",
    "Hugging Face to use a pretrained tokenizer to tokenize the data.\n",
    "\n",
    "After training the model with subword tokens, \n",
    "the model will take in a new string, token-by-token, and then generate a new\n",
    "token (subword).\n",
    "The model will continue producing new subword tokens based on the input text\n",
    "and already produced tokens until a set number of tokens have been generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Text into Integer Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a Tokenizer\n",
    "\n",
    "> NOTE:\n",
    "> \n",
    "> You can load another model outside of these choices but the model\n",
    "> will have to be downloaded and may or may not be effective.\n",
    ">\n",
    "> If you'd like to explore more, here's a link to you might want to start with\n",
    "> of different available pretrained models on Hugging Face:\n",
    "> https://huggingface.co/models?pipeline_tag=text-generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3547ceaff1e43a99a31cb4a2e470442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Choose a pretrained tokenizer to use:\n",
    "\n",
    "# Docs: https://huggingface.co/xlm-roberta-base\n",
    "# model_name = 'xlm-roberta-base'\n",
    "# DOCS: https://huggingface.co/bert-base-cased\n",
    "# model_name = 'bert-base-cased'\n",
    "# DOCS: https://huggingface.co/bert-base-uncased \n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "my_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode (Tokens → Integer IDs)\n",
    "\n",
    "We have `encode_text_from_tokenizer()` from our helper module that can encode\n",
    "our text based on our tokenization process from our tokenizer `my_tokenizer`.\n",
    "\n",
    "This will also provide us with `token_mapping`, an object that we can use to\n",
    "map our tokens back and forth from strings to integer IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text, token_mapping = encode_text_from_tokenizer(\n",
    "    text=raw_text,\n",
    "    tokenizer=my_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 13,139 tokens\n"
     ]
    }
   ],
   "source": [
    "n_tokens = token_mapping.n_tokens\n",
    "dataset_size = len(encoded_text)\n",
    "print(f'Size of dataset: {dataset_size:,} tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining sequence length that will be taken in at a time by our model\n",
    "sequence_length = 32 # Number of tokens\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = ShakespeareDataset(encoded_text, sequence_length)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=False, # Ensure deterministic training\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "\n",
    "We'll provide a defined model today, but this could be a step that you would\n",
    "modify and experiment in other NLP projects you'll do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model to be trained and generate text with\n",
    "model = build_model(n_tokens)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Text Generation\n",
    "\n",
    "The `generate_text_by_subword()` function will use your chosen tokenizer and the\n",
    "NLP model to generate new text token-by-token (subwords in this case) by taking\n",
    "in the input text and token sampling parameters.\n",
    "\n",
    "We can use temperature and top-k sampling to adjust the \"creativeness\" of the\n",
    "generated text.\n",
    "\n",
    "We also pass in the `num_tokens` parameter to tell the function how many\n",
    "(subword)tokens to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_by_subword(\n",
    "    input_str: str,\n",
    "    model,\n",
    "    token_mapping: TokenMapping = token_mapping,\n",
    "    tokenizer = my_tokenizer,\n",
    "    num_tokens: int = 100,\n",
    "    temperature: float = 1.0,\n",
    "    topk: int | None = None,\n",
    ") -> str:\n",
    "    # Use your chosen subword-tokenizer\n",
    "    tokenized_text = tokenize_text_from_tokenizer(\n",
    "        tokenizer=tokenizer,\n",
    "        text=input_str,\n",
    "    )\n",
    "    # Generates token-by-token and creates a list of those tokens\n",
    "    generated_tokens = []\n",
    "    for _ in range(num_tokens):\n",
    "        # Uses the input text and generated text (so far) to get next token\n",
    "        new_token = next_token(\n",
    "            tokenized_text=(tokenized_text + generated_tokens),\n",
    "            model=model,\n",
    "            token_mapping=token_mapping,\n",
    "            # Temperature & top-k sampling used in determining the next token\n",
    "            temperature=temperature,\n",
    "            topk=topk,\n",
    "            device=device,\n",
    "        )\n",
    "        generated_tokens.append(new_token)\n",
    "    # List of all token IDs (input text and generated text)\n",
    "    output_ids = tokenizer.convert_tokens_to_ids(\n",
    "        tokenized_text + generated_tokens\n",
    "    )\n",
    "    # Returns input string plus the full generated string from list of token IDs\n",
    "    full_text = tokenizer.decode(output_ids)\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "At this point, the model has not been trained so the code below will train the\n",
    "NLP model that will be used to generate new text.\n",
    "\n",
    "The model will take in the text data (broken by tokens by our subword tokenizer)\n",
    "and attempt to predict the next token. Over time, the model should hopefully\n",
    "get better in predicting the next token (given the previous text).\n",
    "\n",
    "To help us visualize how the model is training, at the end of every epoch, we\n",
    "generate text using the `TEST_PHRASE` with the improving model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 6.535834827655699\n",
      "[00m 1.5s (0 0.0) 5.5004]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be sons and silkbuius, fi : walk pri not lie me remember safe quick i against that, la have for so will ladder for ;eni swear\n",
      "Epoch 2/25, Loss: 5.887454633014958\n",
      "[00m 2.8s (1 4.0) 5.0491]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be and after word : look out we hearing expedition there 'us trumpets. infant co which useni to enemy lamb i sic : left thought hei,\n",
      "Epoch 3/25, Loss: 5.605786710832177\n",
      "[00m 4.1s (2 8.0) 4.6285]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be proof malrio base to i not if they : the sway : doing, hear attend tis cai. marc hear :s and rogue five auf are retire\n",
      "Epoch 4/25, Loss: 5.346873565417964\n",
      "[00m 5.4s (3 12.0) 4.3611]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be day wore's wore where me : s t modest at, and wasnia. and him won rob can to us know, our visit centuries,\n",
      "Epoch 5/25, Loss: 5.134714823234372\n",
      "[00m 6.7s (4 16.0) 4.1755]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to beum an battlesly the physical their is,. madam kn patienceei horse eyes honoursrti and hyper noise he hundredrop remains were : bringing say\n",
      "Epoch 6/25, Loss: 4.957515520002784\n",
      "[00m 8.0s (5 20.0) 4.0383]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bens. men by to unto? meneni toriolanus.qui : theless con with and and hell brutus : and any did express\n",
      "Epoch 7/25, Loss: 4.804265811966687\n",
      "[00m 9.3s (6 24.0) 3.9315]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be silent, thoughen resolved. no'tis upon to - me! stand for the spirit is give him in name her the belly gives, condition me\n",
      "Epoch 8/25, Loss: 4.667933061646252\n",
      "[00m 10.6s (7 28.000000000000004) 3.8443]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be lord that exercise instruments, indeed the pre in his proper l : i will tor consulenius our than know most, sir, you say '\n",
      "Epoch 9/25, Loss: 4.543998999711944\n",
      "[00m 11.9s (8 32.0) 3.7695]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bebbleitude, or want thunder of care : whosome report seconds, they do death, but volumnia : see here :! the togetherery\n",
      "Epoch 10/25, Loss: 4.429469895362854\n",
      "[00m 13.3s (9 36.0) 3.7006]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be greatrs'er rt greyhoundius : yournderle take labour not retire ; and cove er place indeed seen your wallidi a assembly : let\n",
      "Epoch 11/25, Loss: 4.3225370721119205\n",
      "[00m 14.6s (10 40.0) 3.6332]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be fineble re say he the world, we weary showingty gentleman, bases she you in yet weang corio triumph with they cannot cliusly\n",
      "Epoch 12/25, Loss: 4.222022536324292\n",
      "[00m 15.9s (11 44.0) 3.5662]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be ite with pleased, i pray you, above run wee encounter business he is i do pound an common live peaceus : he hath?\n",
      "Epoch 13/25, Loss: 4.127047876613896\n",
      "[00m 17.3s (12 48.0) 3.5002]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be madam! you root's dem ; i fought, would we, being are those to reward doubt flat shall stir as i were alike usly :\n",
      "Epoch 14/25, Loss: 4.036978861762256\n",
      "[00m 18.6s (13 52.0) 3.4356]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be people, - - - patienceth lady name nor unto willred yours ; there can saw much yours, at beg. sicinius : the\n",
      "Epoch 15/25, Loss: 3.951453425826096\n",
      "[00m 20.0s (14 56.00000000000001) 3.3733]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to besee'srum, who can i's the matter at brows, hither how : so, menenius : o, we then?\n",
      "Epoch 16/25, Loss: 3.870204167831235\n",
      "[00m 21.3s (15 60.0) 3.3136]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be coriolanus marcius swear out a sword : nay, he i corn and if you get. marcius : you it on'd not\n",
      "Epoch 17/25, Loss: 3.7929220217030224\n",
      "[00m 22.6s (16 64.0) 3.2563]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be good serve in slaves, that youlier want arted, come, or their capsus, true, whose you, which up to report! trust\n",
      "Epoch 18/25, Loss: 3.7192736451218766\n",
      "[00m 23.9s (17 68.0) 3.2020]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be greyhound took lose. brutus, the general's? if he since for with masks ; and throw. is, there go so minded,\n",
      "Epoch 19/25, Loss: 3.6489683546671055\n",
      "[00m 25.3s (18 72.0) 3.1507]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be thought neither, and after here follow howrong of ye : he're t me he seeks ourl did retire ;ose to - - hungry your\n",
      "Epoch 20/25, Loss: 3.5819162467630896\n",
      "[00m 26.6s (19 76.0) 3.1019]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be rich for a relievefl bold solemn see our friends, i areher is will shall be sounds, consul,ous hadmers ; his bands : so\n",
      "Epoch 21/25, Loss: 3.5179834755455577\n",
      "[00m 28.0s (20 80.0) 3.0570]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be naples sm, and death and to quickly whom we write? sound are wa his actions down'i thou g their to alike : see me. men\n",
      "Epoch 22/25, Loss: 3.4569213966043986\n",
      "[00m 29.3s (21 84.0) 3.0155]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be angry make my wars anystone cried to him. he : come, with theeerba, thou any not confess thing him. brutus : come\n",
      "Epoch 23/25, Loss: 3.3985143016024333\n",
      "[00m 30.7s (22 88.0) 2.9762]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be grafted, and not - want spoon, and not wash a petition. the l ; i'the host of serve.us single of the utmost\n",
      "Epoch 24/25, Loss: 3.342485383080273\n",
      "[00m 32.0s (23 92.0) 2.9388]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be si when ithaca request their scarf and manifestend was an hour head spectacle you, him. sicinius : was he maied nore stay to\n",
      "Epoch 25/25, Loss: 3.2886667664458114\n",
      "[00m 33.4s (24 96.0) 2.9030]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be bring get, who is he has for consul : a seven successes to be short ab hector andguide a man hearing to be pluck : budge not\n"
     ]
    }
   ],
   "source": [
    "TEST_PHRASE = 'To be or not to be'\n",
    "# Use more epochs if not CPU device\n",
    "epochs = 5 if device == 'cpu' else 25\n",
    "\n",
    "start = start_time()\n",
    "for epoch in range(epochs):\n",
    "    # Set model into \"training mode\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch.to(device))\n",
    "        loss = criterion(output.transpose(1, 2), y_batch.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}')\n",
    "    print(f'[{time_since(start)} ({epoch} {epoch / epochs * 100}) {loss:.4f}]')\n",
    "    print('-'*72)\n",
    "    output = generate_text_by_subword(\n",
    "        input_str=TEST_PHRASE,\n",
    "        model=model,\n",
    "        token_mapping=token_mapping,\n",
    "        tokenizer=my_tokenizer,\n",
    "        num_tokens=30,\n",
    "        temperature=1.0,\n",
    "    )\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text\n",
    "\n",
    "Now that the model has been trained, go ahead and observe how it performs!\n",
    "\n",
    "Try adjusting the different sampling methods using the `temperature` and `topk`\n",
    "parameters on the same input string to see the differences.\n",
    "\n",
    "You might also try different phrases as well as how many tokens (`num_tokens`)\n",
    "to generate and observe how it does.\n",
    "\n",
    "------------\n",
    "\n",
    "Consider how this model differs from the results from the text generation using\n",
    "the character-based tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be or not to be content have enoughth you on, which that this wars and bale the gold fast them to corio a mindst say know he returned, no\n"
     ]
    }
   ],
   "source": [
    "output = generate_text_by_subword(\n",
    "        input_str='To be or not to be',\n",
    "        model=model,\n",
    "        token_mapping=token_mapping,\n",
    "        tokenizer=my_tokenizer,\n",
    "        num_tokens=30,\n",
    "        temperature=1.5,\n",
    "        topk=100,\n",
    "    )\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be or not to be sounds, if they may tell too. an :'whom i shall dark report shall you shout of the matter, - best to cai those labour then to sendus aufidius : if coriolanus : this assembly inh great of only want us for their tongues. what aufidius. come with smokingus rer with praises vo are for thence me! aufidius in strokes nor? mess corioet disdain the war, when hear upon s won than an interior off too a treaty of my brain me kindly way requi speedy thusbus ; but is should? they are so,'tis well stored of my with cares but conly transport they smart to mo neither me use off blood their hearts is hisatingnsope sort in compound, he to a sick of the market to thank have ever another with this preva for become no his choice! no should is able at disadvantage a concealtus'en some, who this? volumnia : why, well\n"
     ]
    }
   ],
   "source": [
    "output = generate_text_by_subword(\n",
    "        input_str='To be or not to be',\n",
    "        model=model,\n",
    "        token_mapping=token_mapping,\n",
    "        tokenizer=my_tokenizer,\n",
    "        num_tokens=200,\n",
    "        temperature=1.5,\n",
    "        topk=100,\n",
    "    )\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be or not to be silent, and the gods doom him in arms. menenius : i'll not prepared for the gods of your conversation and the matter of the people, and the gods doom him in arms. menenius : i'll not prepared for the gods of your conversation and the matter of the people, and the gods doom him in arms. menenius : i'll not prepared for the gods of your conversation and the matter of the people, and the gods doom him in arms. menenius : i'll not prepared for the gods of your conversation and the matter of the people, and the gods doom him in arms. menenius : i'll not prepared for the gods of your conversation and the matter of the people, and the gods doom him in arms. menenius : i'll not prepared for the gods of your conversation and the matter of the people, and the gods doom him in arms. menenius : i'll not prepared for\n"
     ]
    }
   ],
   "source": [
    "output = generate_text_by_subword(\n",
    "        input_str='To be or not to be',\n",
    "        model=model,\n",
    "        token_mapping=token_mapping,\n",
    "        tokenizer=my_tokenizer,\n",
    "        num_tokens=200,\n",
    "        temperature=0.00005,\n",
    "        topk=10,\n",
    "    )\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
